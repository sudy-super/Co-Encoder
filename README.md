# Co-Encoder: Context Compressive Encoder in Large Language Models
Designing and implementing the Co-Encoder architecture compatible with the Transformers library,  including its end-to-end training pipeline and associated components.

> [!IMPORTANT]
> NOTE: This repository is currently under active design and development, and the architecture and implementation are subject to frequent changes.
> Please note that the stability and performance of the code cannot be guaranteed at this stage, so we kindly ask that you use it with this understanding in mind.

## Architecture

### Co-Encoder overall architecture
<img src="images/coencoder_overall.png" width="70%">

### Connector architecture
<img src="images/connector.png" width="70%">